
# Jupyter Notebook ì„¤ì •
- Image : `jupyter-tensorflow-full:v1.9.2`
- ì„¤ì¹˜ Python ë¼ì´ë¸ŒëŸ¬ë¦¬

```py
!pip install onnx==1.14.1
!pip install onnxruntime==1.20.1
!pip install tf2onnx==1.16.1
!pip install model-registry==0.2.10
!pip install boto3==1.36.5
```

# Model í•™ìŠµ
```py
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense,Flatten
import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score
from datetime import datetime
import tf2onnx
import onnx
import onnxruntime as ort
import os
import boto3

# MNIST ë°ì´í„°ì…‹ ë¡œë“œ
(X_train,y_train),(X_test,y_test) = keras.datasets.mnist.load_data()

X_train = X_train/255
X_test = X_test/255

# ëª¨ë¸ Training
model = Sequential()

model.add(Flatten(input_shape=(28,28)))
model.add(Dense(128,activation='relu'))
model.add(Dense(32,activation='relu'))
model.add(Dense(10,activation='softmax'))

model.compile(loss='sparse_categorical_crossentropy',optimizer='Adam',metrics=['accuracy'])
model.summary()

##############################################################################

# ëª¨ë¸ í›ˆë ¨
history = model.fit(X_train, y_train, epochs=10, validation_split=0.2)

##############################################################################

# í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ëŒ€í•œ ì˜ˆì¸¡
y_prob = model.predict(X_test)
y_pred = y_prob.argmax(axis=1)  # ê°€ì¥ ë†’ì€ í™•ë¥ ì˜ í´ë˜ìŠ¤ ì„ íƒ
accuracy = accuracy_score(y_test, y_pred)  # ì •í™•ë„ ê³„ì‚°
print("ì •í™•ë„: ", accuracy)  # ì •í™•ë„ ì¶œë ¥

# ì²« ë²ˆì§¸ í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ì‹œê°í™”
plt.imshow(X_test[4], cmap="Greys")
plt.title('X_test[4]:')
plt.show()

# íŠ¹ì • í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ì— ëŒ€í•œ ì˜ˆì¸¡
prediction = model.predict(X_test[4].reshape(1, 28, 28)).argmax(axis=1)[0]
print("íŠ¹ì • ì´ë¯¸ì§€ ì˜ˆì¸¡ ê²°ê³¼: ", prediction)

##############################################################################

# Keras ëª¨ë¸ì„ ONNX ëª¨ë¸ë¡œ ë³€í™˜ ì €ì¥
input_signature = [tf.TensorSpec([1, 28, 28], tf.double, name='x')]
onnx_model, _ = tf2onnx.convert.from_keras(model, input_signature, opset=12)

model_file_name = "model1.onnx"
onnx.save(onnx_model, model_file_name)  # ONNX ëª¨ë¸ ì €ì¥

##############################################################################

# ë³€í™˜ëœ ONNX ëª¨ë¸ ë¡œë“œ
onnx_model = onnx.load(model_file_name)

# ì…ë ¥ ì°¨ì› í™•ì¸
input_tensor = onnx_model.graph.input[0]  # ì²« ë²ˆì§¸ ì…ë ¥ í…ì„œ
input_name = input_tensor.name
input_shape = [dim.dim_value for dim in input_tensor.type.tensor_type.shape.dim]

# ì¶œë ¥ ì°¨ì› í™•ì¸
output_tensor = onnx_model.graph.output[0]  # ì²« ë²ˆì§¸ ì¶œë ¥ í…ì„œ
output_name = output_tensor.name
output_shape = [dim.dim_value for dim in output_tensor.type.tensor_type.shape.dim]

print(f"ì…ë ¥ í…ì„œ ì´ë¦„: {input_name}, ì…ë ¥ ì°¨ì›: {input_shape}")
print(f"ì¶œë ¥ í…ì„œ ì´ë¦„: {output_name}, ì¶œë ¥ ì°¨ì›: {output_shape}")

# ONNX ëª¨ë¸ì„ ì‚¬ìš©í•œ ì¶”ë¡ 
sess = ort.InferenceSession(model_file_name)
results_ort = sess.run([output_name], {'x': X_test[4].reshape(input_shape)})
prediction = results_ort[0].argmax(axis=1)[0]  # ì˜ˆì¸¡ í´ë˜ìŠ¤ ì„ íƒ
print('prediction: ', prediction)

##############################################################################
```

# Model Artifact S3(MinIO)ì— ì €ì¥
- Triton Server ë¡œ ë°°í¬ë˜ëŠ” ëª¨ë¸ì€ ì•„ë˜ì™€ ê°™ì€ í´ë” êµ¬ì¡°ë¥¼ ê°€ì ¸ì•¼ë§Œ í•¨
- ìì„¸í•œ ë‚´ìš©ì€ ë§í¬ ì°¸ê³  (https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/user_guide/model_repository.html)
```bash
#TensorRT model repository
  <model-repository-path>/
    <model-name>/
      config.pbtxt
      1/
        model.plan
         
#ONNX Model repository
  <model-repository-path>/
    <model-name>/
      config.pbtxt
      1/
        model.onnx
         
#TorchScript repository
  <model-repository-path>/
    <model-name>/
      config.pbtxt
      1/
        model.pt
         
#Tensorflow GraphDef Model repository
  <model-repository-path>/
    <model-name>/
      config.pbtxt
      1/
        model.graphdef
 
#Tensorflow GraphDef Model repository
  <model-repository-path>/
    <model-name>/
      config.pbtxt
      1/
        model.savedmodel/
           <saved-model files>

#Example
 ai-project/
  onnx-model-test/
   config.pbtxt
   1/               # ë²„ì „í´ë” : ì •ìˆ˜ë¡œë§Œ ê´€ë¦¬ê°€ëŠ¥ (ex. 1,2,3..)
    model.onnx

# ğŸš¨ ì˜ëª»ëœ ì˜ˆì‹œ (ì‚¬ìš© ë¶ˆê°€)
model-repository/
â”œâ”€â”€ my_model/
â”‚   â”œâ”€â”€ 0/         # âŒ ë²„ì „ 0 â†’ Tritonì—ì„œ ì¸ì‹í•˜ì§€ ëª»í•¨
â”‚   â”œâ”€â”€ 1.1/       # âŒ ë²„ì „ì´ ì‹¤ìˆ˜(float) â†’ ì§€ì›ë˜ì§€ ì•ŠìŒ
â”‚   â”œâ”€â”€ latest/    # âŒ ë¬¸ìì—´ ë²„ì „ â†’ ì§€ì›ë˜ì§€ ì•ŠìŒ
â”‚   â”œâ”€â”€ v2/        # âŒ v2 ê°™ì€ ë¬¸ìì—´ ë²„ì „ â†’ ì§€ì›ë˜ì§€ ì•ŠìŒ
â”‚   â”œâ”€â”€ model.onnx # âŒ ëª¨ë¸ì´ ë£¨íŠ¸ í´ë”ì— ìˆìŒ â†’ ì˜ëª»ëœ êµ¬ì¡°

# âœ… ì˜¬ë°”ë¥¸ ì˜ˆì‹œ (ì‚¬ìš© ê°€ëŠ¥)
model-repository/
â”œâ”€â”€ my_model/
â”‚   â”œâ”€â”€ 1/        # âœ… ì˜¬ë°”ë¥¸ ë²„ì „ í´ë”
â”‚   â”‚   â”œâ”€â”€ model.onnx
â”‚   â”œâ”€â”€ 2/        # âœ… ì˜¬ë°”ë¥¸ ë²„ì „ í´ë”
â”‚   â”‚   â”œâ”€â”€ model.onnx
â”‚   â”œâ”€â”€ config.pbtxt
```

- Model Object Storageì— ì €ì¥
```py
os.environ['AWS_ACCESS_KEY_ID'] = 'modelregistry'
os.environ['AWS_SECRET_ACCESS_KEY'] = 'modelregistry1!'

project_name = "ai-project"
model_name = "onnx-model-test"
version_name = "2"
artifact_name = "model.onnx"

s3 = boto3.resource(
    service_name='s3',
    region_name='default',
    endpoint_url='http://minio.minio-system.svc.cluster.local:9000',
    use_ssl=False
)

bucket_name = 'model-registry-bucket'
bucket_storage_key = f'aws-connection-{bucket_name}'
bucket_storage_path = f'{project_name}/{model_name}/{version_name}/{artifact_name}'
full_bucket_target = f's3://{bucket_name}/{project_name}/{model_name}/{version_name}/{artifact_name}'

my_bucket = s3.Bucket(bucket_name)
my_bucket.upload_file(model_file_name, bucket_storage_path)
```

- config.pbtxt íŒŒì¼ì€ ëª¨ë¸ ì €ì¥ í´ë” ìµœìƒë‹¨ì— ìœ„ì¹˜
- ìì„¸í•œ ë‚´ìš©ì€ ë§í¬ ì°¸ê³  (https://github.com/triton-inference-server/server/blob/main/docs/user_guide/model_configuration.md)

```py
# config.pbtxt íŒŒì¼ ì—…ë¡œë“œ í•„ìˆ˜
config_file_name = "config.pbtxt"
with open(config_file_name, 'w') as f:
    f.write(f"""
name: "{model_name}"
platform: "onnxruntime_onnx"
max_batch_size: 0

input [
  {{
    name: "{input_name}"  
    data_type: TYPE_FP64  
    dims: {input_shape}
  }}
]

output [
  {{
    name: "{output_name}"  
    data_type: TYPE_FP32
    dims: {output_shape}
  }}
]
""")

# MinIO(S3)ë¡œ config.pbtxt ì—…ë¡œë“œ
config_storage_path = f'{project_name}/{model_name}/config.pbtxt'
my_bucket.upload_file(config_file_name, config_storage_path)

print("ls:")
for obj in my_bucket.objects.filter():
    print(obj.key)
```

- Datatype

|Model Config  |TensorRT      |TensorFlow    |ONNX Runtime  |PyTorch  |API      |NumPy         |
|--------------|--------------|--------------|--------------|---------|---------|--------------|
|TYPE_BOOL     | kBOOL        |DT_BOOL       |BOOL          |kBool    |BOOL     |bool          |
|TYPE_UINT8    | kUINT8       |DT_UINT8      |UINT8         |kByte    |UINT8    |uint8         |
|TYPE_UINT16   |              |DT_UINT16     |UINT16        |         |UINT16   |uint16        |
|TYPE_UINT32   |              |DT_UINT32     |UINT32        |         |UINT32   |uint32        |
|TYPE_UINT64   |              |DT_UINT64     |UINT64        |         |UINT64   |uint64        |
|TYPE_INT8     | kINT8        |DT_INT8       |INT8          |kChar    |INT8     |int8          |
|TYPE_INT16    |              |DT_INT16      |INT16         |kShort   |INT16    |int16         |
|TYPE_INT32    | kINT32       |DT_INT32      |INT32         |kInt     |INT32    |int32         |
|TYPE_INT64    | kINT64       |DT_INT64      |INT64         |kLong    |INT64    |int64         |
|TYPE_FP16     | kHALF        |DT_HALF       |FLOAT16       |         |FP16     |float16       |
|TYPE_FP32     | kFLOAT       |DT_FLOAT      |FLOAT         |kFloat   |FP32     |float32       |
|TYPE_FP64     |              |DT_DOUBLE     |DOUBLE        |kDouble  |FP64     |float64       |
|TYPE_STRING   |              |DT_STRING     |STRING        |         |BYTES    |dtype(object) |
|TYPE_BF16     | kBF16        |              |              |         |BF16     |              |

# Model Registry ì— ë“±ë¡

```py
from model_registry import ModelRegistry
registry = ModelRegistry("http://model-registry-service.kubeflow.svc.cluster.local", 8080, author="wyseo", is_secure=False)  # insecure port set to 8080

model = registry.register_model(
    model_name,  
    f's3://{bucket_name}/{project_name}',  
    version=version_name,
    description="onnx ëª¨ë¸ ë“±ë¡ í…ŒìŠ¤íŠ¸",
    model_format_name="onnx",    # Model Format --> https://github.com/kserve/modelmesh-serving/tree/main/docs/model-formats
    model_format_version="1",
    storage_key=bucket_storage_key,
    storage_path=bucket_storage_path
)
```

# ë“±ë¡ëœ ëª¨ë¸ ì¡°íšŒ
```py
import requests
import json

mr_hostname = "http://model-registry-service.kubeflow.svc.cluster.local:8080"
model_id = registry.get_registered_model(model_name).id
model_version_id = registry.get_model_version(model_name, version_name).id

def fetch_and_print(url):
    response = requests.get(url, headers={'accept': 'application/json'})
    if response.status_code == 200:
        print(json.dumps(response.json(), indent=2))
    else:
        print(f"Error fetching data from {url}: {response.status_code}")

# ëª¨ë¸ ë²„ì „ ì¡°íšŒ
model_versions_url = f"{mr_hostname}/api/model_registry/v1alpha3/registered_models/{model_id}/versions"
print("\033[91mModel Versions:\033[0m")
fetch_and_print(model_versions_url)
print("\n\n")
# ëª¨ë¸ ë²„ì „ ì•„í‹°íŒ©íŠ¸ ì¡°íšŒ
model_artifacts_url = f"{mr_hostname}/api/model_registry/v1alpha3/model_versions/{model_version_id}/artifacts"
print("\033[91mModel Artifacts:\033[0m")
fetch_and_print(model_artifacts_url)
```

```json
Model Versions:
{
  "items": [
    {
      "author": "wyseo",
      "createTimeSinceEpoch": "1738821693227",
      "customProperties": {},
      "description": "onnx \ubaa8\ub378 \ub4f1\ub85d \ud14c\uc2a4\ud2b8",
      "id": "2",
      "lastUpdateTimeSinceEpoch": "1738821693227",
      "name": "1",
      "registeredModelId": "1",
      "state": "LIVE"
    }
  ],
  "nextPageToken": "",
  "pageSize": 0,
  "size": 1
}



Model Artifacts:
{
  "items": [
    {
      "artifactType": "model-artifact",
      "createTimeSinceEpoch": "1738821693295",
      "customProperties": {},
      "id": "1",
      "lastUpdateTimeSinceEpoch": "1738821693295",
      "modelFormatName": "onnx",
      "modelFormatVersion": "1",
      "name": "onnx-model-test",
      "state": "UNKNOWN",
      "storageKey": "aws-connection-model-registry-bucket",
      "storagePath": "ai-project/onnx-model-test/1/model.onnx",
      "uri": "s3://model-registry-bucket/ai-project"
    }
  ],
  "nextPageToken": "",
  "pageSize": 0,
  "size": 1
}
```

# Model KServe ë°°í¬
- ë‘˜ ì¤‘ í•˜ë‚˜ë¡œ ë°°í¬í•˜ë©´ ë¨ (onnx model Formatì€ ê²°êµ­ triton server ëŸ°íƒ€ì„ì„ ì‚¬ìš©í•˜ê²Œ ë¨)

```yaml
apiVersion: "serving.kserve.io/v1beta1"
kind: "InferenceService"
metadata:
  name: inf-onnx-model-test
  namespace: kubeflow-user-example-com
spec:
  predictor:
    serviceAccountName: minio-sa  # MinIOì—ì„œ ëª¨ë¸ì„ ì ‘ê·¼í•  ì„œë¹„ìŠ¤ ê³„ì •
    triton:
      storageUri: "s3://model-registry-bucket/ai-project"
      modelName: "onnx-model-test"  # ëª¨ë¸ ì´ë¦„
      modelVersion: "2"            # ëª¨ë¸ ë²„ì „
      protocolVersion: "v2"  

---

apiVersion: "serving.kserve.io/v1beta1"
kind: "InferenceService"
metadata:
  name: inf-onnx-model-test
  namespace: kubeflow-user-example-com
spec:
  predictor:
    serviceAccountName: minio-sa
    model:
      modelName: "onnx-model-test"
      modelVersion: "1"
      protocolVersion: "v2"    
      modelFormat:
        name: onnx
      storageUri: "s3://model-registry-bucket/ai-project"
```


# ì¶”ë¡  API Test

- Model ë©”íƒ€ë°ì´í„° ì¡°íšŒ

```py
import requests
import json

url = 'http://inf-onnx-model-test.kubeflow-user-example-com.svc.cluster.local/v2/models/onnx-model-test'

response = requests.get(url, headers={'accept': 'application/json'})
if response.status_code == 200:
    print(json.dumps(response.json(), indent=2))
else:
    print(f"Error fetching data from {url}: {response.status_code}")
```

- ì¶”ë¡  ìš”ì²­

```py
import requests
import numpy as np
from PIL import Image
from tensorflow.keras.datasets import mnist
import matplotlib.pyplot as plt

# MNIST ë°ì´í„°ì…‹ ë¡œë“œ
(x_train, y_train), (x_test, y_test) = mnist.load_data()
test_image = x_test[1]  # ì²« ë²ˆì§¸ í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ì„ íƒ

# í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ì „ì²˜ë¦¬
image_array = np.array(test_image).astype(np.float32) / 255.0  # ì •ê·œí™”
image_array = np.expand_dims(image_array, axis=0)  # ë°°ì¹˜ ì°¨ì› ì¶”ê°€

# KServe ì„œë¹„ìŠ¤ ì—”ë“œí¬ì¸íŠ¸ (v2 í”„ë¡œí† ì½œ)
url = 'http://inf-onnx-model-test.kubeflow-user-example-com.svc.cluster.local/v2/models/onnx-model-test/infer'

# ìš”ì²­ ë°ì´í„° ì¤€ë¹„
data = {
    "inputs": [{
        "name": "x",  # ëª¨ë¸ ì…ë ¥ ì´ë¦„
        "shape": image_array.shape,
        "datatype": "FP64",
        "data": image_array.flatten().tolist()  # 1D ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
    }]
}

# API í˜¸ì¶œ
response = requests.post(url, json=data)

# ì˜ˆì¸¡ ê²°ê³¼ ì¶œë ¥ ë° ì‹œê°í™”
if response.status_code == 200:
    result = response.json()
    # print(f"Prediction: {result}")
    
    # ì˜ˆì¸¡ëœ í™•ë¥ 
    probabilities = result['outputs'][0]['data']
    
    # ê°€ì¥ ë†’ì€ í™•ë¥ ì„ ê°€ì§„ í´ë˜ìŠ¤ ì°¾ê¸°
    predicted_class = np.argmax(probabilities)
    predicted_probability = probabilities[predicted_class]
    
    print(f"Predicted Class: {predicted_class}, Probability: {predicted_probability}")

    # ì´ë¯¸ì§€ ì‹œê°í™”
    plt.imshow(test_image, cmap='gray')
    plt.title(f'Predicted Class: {predicted_class} (Probability: {predicted_probability:.4f})')
    plt.axis('off')
    plt.show()

else:
    print(f"Request failed with status code {response.status_code}")
```



# 2ê°œ ë²„ì „ ëª¨ë¸ ì„œë¹™í•˜ê¸°

- ëª¨ë¸ ë²„ì „ ìˆ˜ì • ë° Triton Server Config version_policy ì¶”ê°€
```py
os.environ['AWS_ACCESS_KEY_ID'] = 'modelregistry'
os.environ['AWS_SECRET_ACCESS_KEY'] = 'modelregistry1!'

project_name = "ai-project"
model_name = "onnx-model-test"
# version ìˆ˜ì •
version_name = "2"
artifact_name = "model.onnx"

s3 = boto3.resource(
    service_name='s3',
    region_name='default',
    endpoint_url='http://minio.minio-system.svc.cluster.local:9000',
    use_ssl=False
)

bucket_name = 'model-registry-bucket'
bucket_storage_key = f'aws-connection-{bucket_name}'
bucket_storage_path = f'{project_name}/{model_name}/{version_name}/{artifact_name}'
full_bucket_target = f's3://{bucket_name}/{project_name}/{model_name}/{version_name}/{artifact_name}'

my_bucket = s3.Bucket(bucket_name)
my_bucket.upload_file(model_file_name, bucket_storage_path)

# config.pbtxt íŒŒì¼ ìˆ˜ì • (version_policy ì¶”ê°€)
config_file_name = "config.pbtxt"
with open(config_file_name, 'w') as f:
    f.write(f"""
name: "{model_name}"
platform: "onnxruntime_onnx"
max_batch_size: 0

version_policy: {{
  latest: {{ num_versions: 2 }} 
}}

input [
  {{
    name: "{input_name}"  
    data_type: TYPE_FP64  
    dims: {input_shape}
  }}
]

output [
  {{
    name: "{output_name}"  
    data_type: TYPE_FP32
    dims: {output_shape}
  }}
]
""")

# MinIO(S3)ë¡œ config.pbtxt ì—…ë¡œë“œ
config_storage_path = f'{project_name}/{model_name}/config.pbtxt'
my_bucket.upload_file(config_file_name, config_storage_path)
```
      
- KServe InferenceService ë°°í¬ (`modelVersion` ì œê±°)

```yaml
apiVersion: "serving.kserve.io/v1beta1"
kind: "InferenceService"
metadata:
  name: inf-onnx-model-test
  namespace: kubeflow-user-example-com
spec:
  predictor:
    serviceAccountName: minio-sa
    model:
      modelName: "onnx-model-test"    
      protocolVersion: "v2"    
      modelFormat:
        name: onnx
      storageUri: "s3://model-registry-bucket/ai-project"
```

- ê²°ê³¼ í™•ì¸

```bash
[root@kubeflow ~]# k logs -n kubeflow-user-example-com inf-onnx-model-test-predictor-00001-deployment-74cf95c897-swmmr

=============================
== Triton Inference Server ==
=============================

NVIDIA Release 23.05 (build 61161506)
Triton Server Version 2.34.0

Copyright (c) 2018-2023, NVIDIA CORPORATION & AFFILIATES.  All rights reserved.

Various files include modifications (c) NVIDIA CORPORATION & AFFILIATES.  All rights reserved.

This container image and its contents are governed by the NVIDIA Deep Learning Container License.
By pulling and using the container, you accept the terms and conditions of this license:
https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license

WARNING: The NVIDIA Driver was not detected.  GPU functionality will not be available.
   Use the NVIDIA Container Toolkit to start this container with GPU support; see
   https://docs.nvidia.com/datacenter/cloud-native/ .

W0206 06:33:15.417728 1 pinned_memory_manager.cc:236] Unable to allocate pinned system memory, pinned memory pool will not be available: CUDA driver version is insufficient for CUDA runtime version
I0206 06:33:15.417810 1 cuda_memory_manager.cc:115] CUDA memory pool disabled
I0206 06:33:15.419118 1 model_lifecycle.cc:462] loading: onnx-model-test:1
I0206 06:33:15.419169 1 model_lifecycle.cc:462] loading: onnx-model-test:2
I0206 06:33:15.420559 1 onnxruntime.cc:2504] TRITONBACKEND_Initialize: onnxruntime
I0206 06:33:15.420594 1 onnxruntime.cc:2514] Triton TRITONBACKEND API version: 1.12
I0206 06:33:15.420599 1 onnxruntime.cc:2520] 'onnxruntime' TRITONBACKEND API version: 1.12
I0206 06:33:15.420604 1 onnxruntime.cc:2550] backend configuration:
{"cmdline":{"auto-complete-config":"true","backend-directory":"/opt/tritonserver/backends","min-compute-capability":"6.000000","default-max-batch-size":"4"}}
I0206 06:33:15.434784 1 onnxruntime.cc:2608] TRITONBACKEND_ModelInitialize: onnx-model-test (version 1)
I0206 06:33:15.434828 1 onnxruntime.cc:2608] TRITONBACKEND_ModelInitialize: onnx-model-test (version 2)
I0206 06:33:15.435350 1 onnxruntime.cc:666] skipping model configuration auto-complete for 'onnx-model-test': inputs and outputs already specified
I0206 06:33:15.435355 1 onnxruntime.cc:666] skipping model configuration auto-complete for 'onnx-model-test': inputs and outputs already specified
I0206 06:33:15.435790 1 onnxruntime.cc:2651] TRITONBACKEND_ModelInstanceInitialize: onnx-model-test_0 (CPU device 0)
I0206 06:33:15.435803 1 onnxruntime.cc:2651] TRITONBACKEND_ModelInstanceInitialize: onnx-model-test_0 (CPU device 0)
I0206 06:33:15.442542 1 onnxruntime.cc:2651] TRITONBACKEND_ModelInstanceInitialize: onnx-model-test_1 (CPU device 0)
I0206 06:33:16.076472 1 onnxruntime.cc:2651] TRITONBACKEND_ModelInstanceInitialize: onnx-model-test_1 (CPU device 0)
I0206 06:33:16.476478 1 model_lifecycle.cc:815] successfully loaded 'onnx-model-test'
I0206 06:33:16.476605 1 server.cc:582]
+------------------+------+
| Repository Agent | Path |
+------------------+------+
+------------------+------+

I0206 06:33:16.476653 1 server.cc:609]
+-------------+-----------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Backend     | Path                                                            | Config                                                                                                                                                        |
+-------------+-----------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------+
| onnxruntime | /opt/tritonserver/backends/onnxruntime/libtriton_onnxruntime.so | {"cmdline":{"auto-complete-config":"true","backend-directory":"/opt/tritonserver/backends","min-compute-capability":"6.000000","default-max-batch-size":"4"}} |
+-------------+-----------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------+

I0206 06:33:16.476690 1 server.cc:652]
+-----------------+---------+--------+
| Model           | Version | Status |
+-----------------+---------+--------+
| onnx-model-test | 1       | READY  |
| onnx-model-test | 2       | READY  |
+-----------------+---------+--------+

I0206 06:33:16.476976 1 metrics.cc:701] Collecting CPU metrics
I0206 06:33:16.477197 1 tritonserver.cc:2385]
+----------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Option                           | Value                                                                                                                                                                                                           |
+----------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| server_id                        | triton                                                                                                                                                                                                          |
| server_version                   | 2.34.0                                                                                                                                                                                                          |
| server_extensions                | classification sequence model_repository model_repository(unload_dependents) schedule_policy model_configuration system_shared_memory cuda_shared_memory binary_tensor_data parameters statistics trace logging |
| model_repository_path[0]         | /mnt/models                                                                                                                                                                                                     |
| model_control_mode               | MODE_NONE                                                                                                                                                                                                       |
| strict_model_config              | 0                                                                                                                                                                                                               |
| rate_limit                       | OFF                                                                                                                                                                                                             |
| pinned_memory_pool_byte_size     | 268435456                                                                                                                                                                                                       |
| min_supported_compute_capability | 6.0                                                                                                                                                                                                             |
| strict_readiness                 | 1                                                                                                                                                                                                               |
| exit_timeout                     | 30                                                                                                                                                                                                              |
| cache_enabled                    | 0                                                                                                                                                                                                               |
+----------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

```

- API Call Test (https://kserve.github.io/website/master/modelserving/data_plane/data_plane/#v2-apis)

```py
import requests
import numpy as np
from PIL import Image
from tensorflow.keras.datasets import mnist
import matplotlib.pyplot as plt

# MNIST ë°ì´í„°ì…‹ ë¡œë“œ
(x_train, y_train), (x_test, y_test) = mnist.load_data()
test_image = x_test[1]  # ì²« ë²ˆì§¸ í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ì„ íƒ

# í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ì „ì²˜ë¦¬
image_array = np.array(test_image).astype(np.float32) / 255.0  # ì •ê·œí™”
image_array = np.expand_dims(image_array, axis=0)  # ë°°ì¹˜ ì°¨ì› ì¶”ê°€

# KServe ì„œë¹„ìŠ¤ ì—”ë“œí¬ì¸íŠ¸ (v2 í”„ë¡œí† ì½œ)
# url = 'http://inf-onnx-model-test.kubeflow-user-example-com.svc.cluster.local/v2/models/onnx-model-test/versions/1/infer'
url = 'http://inf-onnx-model-test.kubeflow-user-example-com.svc.cluster.local/v2/models/onnx-model-test/versions/2/infer'

# ìš”ì²­ ë°ì´í„° ì¤€ë¹„
data = {
    "inputs": [{
        "name": "x",  # ëª¨ë¸ ì…ë ¥ ì´ë¦„
        "shape": image_array.shape,
        "datatype": "FP64",
        "data": image_array.flatten().tolist()  # 1D ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
    }]
}

# API í˜¸ì¶œ
response = requests.post(url, json=data)

# ì˜ˆì¸¡ ê²°ê³¼ ì¶œë ¥ ë° ì‹œê°í™”
if response.status_code == 200:
    result = response.json()
    # print(f"Prediction: {result}")
    
    # ì˜ˆì¸¡ëœ í™•ë¥ 
    probabilities = result['outputs'][0]['data']
    
    # ê°€ì¥ ë†’ì€ í™•ë¥ ì„ ê°€ì§„ í´ë˜ìŠ¤ ì°¾ê¸°
    predicted_class = np.argmax(probabilities)
    predicted_probability = probabilities[predicted_class]
    
    print(f"Predicted Class: {predicted_class}, Probability: {predicted_probability}")

    # ì´ë¯¸ì§€ ì‹œê°í™”
    plt.imshow(test_image, cmap='gray')
    plt.title(f'Predicted Class: {predicted_class} (Probability: {predicted_probability:.4f})')
    plt.axis('off')
    plt.show()

else:
    print(f"Request failed with status code {response.status_code}")
```
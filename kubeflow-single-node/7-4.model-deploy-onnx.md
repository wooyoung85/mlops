
# Jupyter Notebook 설정
- Image : `jupyter-tensorflow-full:v1.9.2`
- 설치 Python 라이브러리

```py
!pip install onnx==1.14.1
!pip install onnxruntime==1.20.1
!pip install tf2onnx==1.16.1
!pip install model-registry==0.2.10
!pip install boto3==1.36.5
```

# Model 학습
```py
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense,Flatten
import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score
from datetime import datetime
import tf2onnx
import onnx
import onnxruntime as ort
import os
import boto3

# MNIST 데이터셋 로드
(X_train,y_train),(X_test,y_test) = keras.datasets.mnist.load_data()

X_train = X_train/255
X_test = X_test/255

# 모델 Training
model = Sequential()

model.add(Flatten(input_shape=(28,28)))
model.add(Dense(128,activation='relu'))
model.add(Dense(32,activation='relu'))
model.add(Dense(10,activation='softmax'))

model.compile(loss='sparse_categorical_crossentropy',optimizer='Adam',metrics=['accuracy'])
model.summary()

##############################################################################

# 모델 훈련
history = model.fit(X_train, y_train, epochs=10, validation_split=0.2)

##############################################################################

# 테스트 데이터에 대한 예측
y_prob = model.predict(X_test)
y_pred = y_prob.argmax(axis=1)  # 가장 높은 확률의 클래스 선택
accuracy = accuracy_score(y_test, y_pred)  # 정확도 계산
print("정확도: ", accuracy)  # 정확도 출력

# 첫 번째 테스트 이미지 시각화
plt.imshow(X_test[4], cmap="Greys")
plt.title('X_test[4]:')
plt.show()

# 특정 테스트 이미지에 대한 예측
prediction = model.predict(X_test[4].reshape(1, 28, 28)).argmax(axis=1)[0]
print("특정 이미지 예측 결과: ", prediction)

##############################################################################

# Keras 모델을 ONNX 모델로 변환 저장
input_signature = [tf.TensorSpec([1, 28, 28], tf.double, name='x')]
onnx_model, _ = tf2onnx.convert.from_keras(model, input_signature, opset=12)

model_file_name = "model1.onnx"
onnx.save(onnx_model, model_file_name)  # ONNX 모델 저장

##############################################################################

# 변환된 ONNX 모델 로드
onnx_model = onnx.load(model_file_name)

# 입력 차원 확인
input_tensor = onnx_model.graph.input[0]  # 첫 번째 입력 텐서
input_name = input_tensor.name
input_shape = [dim.dim_value for dim in input_tensor.type.tensor_type.shape.dim]

# 출력 차원 확인
output_tensor = onnx_model.graph.output[0]  # 첫 번째 출력 텐서
output_name = output_tensor.name
output_shape = [dim.dim_value for dim in output_tensor.type.tensor_type.shape.dim]

print(f"입력 텐서 이름: {input_name}, 입력 차원: {input_shape}")
print(f"출력 텐서 이름: {output_name}, 출력 차원: {output_shape}")

# ONNX 모델을 사용한 추론
sess = ort.InferenceSession(model_file_name)
results_ort = sess.run([output_name], {'x': X_test[4].reshape(input_shape)})
prediction = results_ort[0].argmax(axis=1)[0]  # 예측 클래스 선택
print('prediction: ', prediction)

##############################################################################
```

# Model Artifact S3(MinIO)에 저장
- Triton Server 로 배포되는 모델은 아래와 같은 폴더 구조를 가져야만 함
- 자세한 내용은 링크 참고 (https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/user_guide/model_repository.html)
```bash
#TensorRT model repository
  <model-repository-path>/
    <model-name>/
      config.pbtxt
      1/
        model.plan
         
#ONNX Model repository
  <model-repository-path>/
    <model-name>/
      config.pbtxt
      1/
        model.onnx
         
#TorchScript repository
  <model-repository-path>/
    <model-name>/
      config.pbtxt
      1/
        model.pt
         
#Tensorflow GraphDef Model repository
  <model-repository-path>/
    <model-name>/
      config.pbtxt
      1/
        model.graphdef
 
#Tensorflow GraphDef Model repository
  <model-repository-path>/
    <model-name>/
      config.pbtxt
      1/
        model.savedmodel/
           <saved-model files>

#Example
 ai-project/
  onnx-model-test/
   config.pbtxt
   1/               # 버전폴더 : 정수로만 관리가능 (ex. 1,2,3..)
    model.onnx

# 🚨 잘못된 예시 (사용 불가)
model-repository/
├── my_model/
│   ├── 0/         # ❌ 버전 0 → Triton에서 인식하지 못함
│   ├── 1.1/       # ❌ 버전이 실수(float) → 지원되지 않음
│   ├── latest/    # ❌ 문자열 버전 → 지원되지 않음
│   ├── v2/        # ❌ v2 같은 문자열 버전 → 지원되지 않음
│   ├── model.onnx # ❌ 모델이 루트 폴더에 있음 → 잘못된 구조

# ✅ 올바른 예시 (사용 가능)
model-repository/
├── my_model/
│   ├── 1/        # ✅ 올바른 버전 폴더
│   │   ├── model.onnx
│   ├── 2/        # ✅ 올바른 버전 폴더
│   │   ├── model.onnx
│   ├── config.pbtxt
```

- Model Object Storage에 저장
```py
os.environ['AWS_ACCESS_KEY_ID'] = 'modelregistry'
os.environ['AWS_SECRET_ACCESS_KEY'] = 'modelregistry1!'

project_name = "ai-project"
model_name = "onnx-model-test"
version_name = "2"
artifact_name = "model.onnx"

s3 = boto3.resource(
    service_name='s3',
    region_name='default',
    endpoint_url='http://minio.minio-system.svc.cluster.local:9000',
    use_ssl=False
)

bucket_name = 'model-registry-bucket'
bucket_storage_key = f'aws-connection-{bucket_name}'
bucket_storage_path = f'{project_name}/{model_name}/{version_name}/{artifact_name}'
full_bucket_target = f's3://{bucket_name}/{project_name}/{model_name}/{version_name}/{artifact_name}'

my_bucket = s3.Bucket(bucket_name)
my_bucket.upload_file(model_file_name, bucket_storage_path)
```

- config.pbtxt 파일은 모델 저장 폴더 최상단에 위치
- 자세한 내용은 링크 참고 (https://github.com/triton-inference-server/server/blob/main/docs/user_guide/model_configuration.md)

```py
# config.pbtxt 파일 업로드 필수
config_file_name = "config.pbtxt"
with open(config_file_name, 'w') as f:
    f.write(f"""
name: "{model_name}"
platform: "onnxruntime_onnx"
max_batch_size: 0

input [
  {{
    name: "{input_name}"  
    data_type: TYPE_FP64  
    dims: {input_shape}
  }}
]

output [
  {{
    name: "{output_name}"  
    data_type: TYPE_FP32
    dims: {output_shape}
  }}
]
""")

# MinIO(S3)로 config.pbtxt 업로드
config_storage_path = f'{project_name}/{model_name}/config.pbtxt'
my_bucket.upload_file(config_file_name, config_storage_path)

print("ls:")
for obj in my_bucket.objects.filter():
    print(obj.key)
```

- Datatype

|Model Config  |TensorRT      |TensorFlow    |ONNX Runtime  |PyTorch  |API      |NumPy         |
|--------------|--------------|--------------|--------------|---------|---------|--------------|
|TYPE_BOOL     | kBOOL        |DT_BOOL       |BOOL          |kBool    |BOOL     |bool          |
|TYPE_UINT8    | kUINT8       |DT_UINT8      |UINT8         |kByte    |UINT8    |uint8         |
|TYPE_UINT16   |              |DT_UINT16     |UINT16        |         |UINT16   |uint16        |
|TYPE_UINT32   |              |DT_UINT32     |UINT32        |         |UINT32   |uint32        |
|TYPE_UINT64   |              |DT_UINT64     |UINT64        |         |UINT64   |uint64        |
|TYPE_INT8     | kINT8        |DT_INT8       |INT8          |kChar    |INT8     |int8          |
|TYPE_INT16    |              |DT_INT16      |INT16         |kShort   |INT16    |int16         |
|TYPE_INT32    | kINT32       |DT_INT32      |INT32         |kInt     |INT32    |int32         |
|TYPE_INT64    | kINT64       |DT_INT64      |INT64         |kLong    |INT64    |int64         |
|TYPE_FP16     | kHALF        |DT_HALF       |FLOAT16       |         |FP16     |float16       |
|TYPE_FP32     | kFLOAT       |DT_FLOAT      |FLOAT         |kFloat   |FP32     |float32       |
|TYPE_FP64     |              |DT_DOUBLE     |DOUBLE        |kDouble  |FP64     |float64       |
|TYPE_STRING   |              |DT_STRING     |STRING        |         |BYTES    |dtype(object) |
|TYPE_BF16     | kBF16        |              |              |         |BF16     |              |

# Model Registry 에 등록

```py
from model_registry import ModelRegistry
registry = ModelRegistry("http://model-registry-service.kubeflow.svc.cluster.local", 8080, author="wyseo", is_secure=False)  # insecure port set to 8080

model = registry.register_model(
    model_name,  
    f's3://{bucket_name}/{project_name}',  
    version=version_name,
    description="onnx 모델 등록 테스트",
    model_format_name="onnx",    # Model Format --> https://github.com/kserve/modelmesh-serving/tree/main/docs/model-formats
    model_format_version="1",
    storage_key=bucket_storage_key,
    storage_path=bucket_storage_path
)
```

# 등록된 모델 조회
```py
import requests
import json

mr_hostname = "http://model-registry-service.kubeflow.svc.cluster.local:8080"
model_id = registry.get_registered_model(model_name).id
model_version_id = registry.get_model_version(model_name, version_name).id

def fetch_and_print(url):
    response = requests.get(url, headers={'accept': 'application/json'})
    if response.status_code == 200:
        print(json.dumps(response.json(), indent=2))
    else:
        print(f"Error fetching data from {url}: {response.status_code}")

# 모델 버전 조회
model_versions_url = f"{mr_hostname}/api/model_registry/v1alpha3/registered_models/{model_id}/versions"
print("\033[91mModel Versions:\033[0m")
fetch_and_print(model_versions_url)
print("\n\n")
# 모델 버전 아티팩트 조회
model_artifacts_url = f"{mr_hostname}/api/model_registry/v1alpha3/model_versions/{model_version_id}/artifacts"
print("\033[91mModel Artifacts:\033[0m")
fetch_and_print(model_artifacts_url)
```

```json
Model Versions:
{
  "items": [
    {
      "author": "wyseo",
      "createTimeSinceEpoch": "1738821693227",
      "customProperties": {},
      "description": "onnx \ubaa8\ub378 \ub4f1\ub85d \ud14c\uc2a4\ud2b8",
      "id": "2",
      "lastUpdateTimeSinceEpoch": "1738821693227",
      "name": "1",
      "registeredModelId": "1",
      "state": "LIVE"
    }
  ],
  "nextPageToken": "",
  "pageSize": 0,
  "size": 1
}



Model Artifacts:
{
  "items": [
    {
      "artifactType": "model-artifact",
      "createTimeSinceEpoch": "1738821693295",
      "customProperties": {},
      "id": "1",
      "lastUpdateTimeSinceEpoch": "1738821693295",
      "modelFormatName": "onnx",
      "modelFormatVersion": "1",
      "name": "onnx-model-test",
      "state": "UNKNOWN",
      "storageKey": "aws-connection-model-registry-bucket",
      "storagePath": "ai-project/onnx-model-test/1/model.onnx",
      "uri": "s3://model-registry-bucket/ai-project"
    }
  ],
  "nextPageToken": "",
  "pageSize": 0,
  "size": 1
}
```

# Model KServe 배포
- 둘 중 하나로 배포하면 됨 (onnx model Format은 결국 triton server 런타임을 사용하게 됨)

```yaml
apiVersion: "serving.kserve.io/v1beta1"
kind: "InferenceService"
metadata:
  name: inf-onnx-model-test
  namespace: kubeflow-user-example-com
spec:
  predictor:
    serviceAccountName: minio-sa  # MinIO에서 모델을 접근할 서비스 계정
    triton:
      storageUri: "s3://model-registry-bucket/ai-project"
      modelName: "onnx-model-test"  # 모델 이름
      modelVersion: "2"            # 모델 버전
      protocolVersion: "v2"  

---

apiVersion: "serving.kserve.io/v1beta1"
kind: "InferenceService"
metadata:
  name: inf-onnx-model-test
  namespace: kubeflow-user-example-com
spec:
  predictor:
    serviceAccountName: minio-sa
    model:
      modelName: "onnx-model-test"
      modelVersion: "1"
      protocolVersion: "v2"    
      modelFormat:
        name: onnx
      storageUri: "s3://model-registry-bucket/ai-project"
```


# 추론 API Test

- Model 메타데이터 조회

```py
import requests
import json

url = 'http://inf-onnx-model-test.kubeflow-user-example-com.svc.cluster.local/v2/models/onnx-model-test'

response = requests.get(url, headers={'accept': 'application/json'})
if response.status_code == 200:
    print(json.dumps(response.json(), indent=2))
else:
    print(f"Error fetching data from {url}: {response.status_code}")
```

- 추론 요청

```py
import requests
import numpy as np
from PIL import Image
from tensorflow.keras.datasets import mnist
import matplotlib.pyplot as plt

# MNIST 데이터셋 로드
(x_train, y_train), (x_test, y_test) = mnist.load_data()
test_image = x_test[1]  # 첫 번째 테스트 이미지 선택

# 테스트 이미지 전처리
image_array = np.array(test_image).astype(np.float32) / 255.0  # 정규화
image_array = np.expand_dims(image_array, axis=0)  # 배치 차원 추가

# KServe 서비스 엔드포인트 (v2 프로토콜)
url = 'http://inf-onnx-model-test.kubeflow-user-example-com.svc.cluster.local/v2/models/onnx-model-test/infer'

# 요청 데이터 준비
data = {
    "inputs": [{
        "name": "x",  # 모델 입력 이름
        "shape": image_array.shape,
        "datatype": "FP64",
        "data": image_array.flatten().tolist()  # 1D 리스트로 변환
    }]
}

# API 호출
response = requests.post(url, json=data)

# 예측 결과 출력 및 시각화
if response.status_code == 200:
    result = response.json()
    # print(f"Prediction: {result}")
    
    # 예측된 확률
    probabilities = result['outputs'][0]['data']
    
    # 가장 높은 확률을 가진 클래스 찾기
    predicted_class = np.argmax(probabilities)
    predicted_probability = probabilities[predicted_class]
    
    print(f"Predicted Class: {predicted_class}, Probability: {predicted_probability}")

    # 이미지 시각화
    plt.imshow(test_image, cmap='gray')
    plt.title(f'Predicted Class: {predicted_class} (Probability: {predicted_probability:.4f})')
    plt.axis('off')
    plt.show()

else:
    print(f"Request failed with status code {response.status_code}")
```



# 2개 버전 모델 서빙하기

- 모델 버전 수정 및 Triton Server Config version_policy 추가
```py
os.environ['AWS_ACCESS_KEY_ID'] = 'modelregistry'
os.environ['AWS_SECRET_ACCESS_KEY'] = 'modelregistry1!'

project_name = "ai-project"
model_name = "onnx-model-test"
# version 수정
version_name = "2"
artifact_name = "model.onnx"

s3 = boto3.resource(
    service_name='s3',
    region_name='default',
    endpoint_url='http://minio.minio-system.svc.cluster.local:9000',
    use_ssl=False
)

bucket_name = 'model-registry-bucket'
bucket_storage_key = f'aws-connection-{bucket_name}'
bucket_storage_path = f'{project_name}/{model_name}/{version_name}/{artifact_name}'
full_bucket_target = f's3://{bucket_name}/{project_name}/{model_name}/{version_name}/{artifact_name}'

my_bucket = s3.Bucket(bucket_name)
my_bucket.upload_file(model_file_name, bucket_storage_path)

# config.pbtxt 파일 수정 (version_policy 추가)
config_file_name = "config.pbtxt"
with open(config_file_name, 'w') as f:
    f.write(f"""
name: "{model_name}"
platform: "onnxruntime_onnx"
max_batch_size: 0

version_policy: {{
  latest: {{ num_versions: 2 }} 
}}

input [
  {{
    name: "{input_name}"  
    data_type: TYPE_FP64  
    dims: {input_shape}
  }}
]

output [
  {{
    name: "{output_name}"  
    data_type: TYPE_FP32
    dims: {output_shape}
  }}
]
""")

# MinIO(S3)로 config.pbtxt 업로드
config_storage_path = f'{project_name}/{model_name}/config.pbtxt'
my_bucket.upload_file(config_file_name, config_storage_path)
```
      
- KServe InferenceService 배포 (`modelVersion` 제거)

```yaml
apiVersion: "serving.kserve.io/v1beta1"
kind: "InferenceService"
metadata:
  name: inf-onnx-model-test
  namespace: kubeflow-user-example-com
spec:
  predictor:
    serviceAccountName: minio-sa
    model:
      modelName: "onnx-model-test"    
      protocolVersion: "v2"    
      modelFormat:
        name: onnx
      storageUri: "s3://model-registry-bucket/ai-project"
```

- 결과 확인

```bash
[root@kubeflow ~]# k logs -n kubeflow-user-example-com inf-onnx-model-test-predictor-00001-deployment-74cf95c897-swmmr

=============================
== Triton Inference Server ==
=============================

NVIDIA Release 23.05 (build 61161506)
Triton Server Version 2.34.0

Copyright (c) 2018-2023, NVIDIA CORPORATION & AFFILIATES.  All rights reserved.

Various files include modifications (c) NVIDIA CORPORATION & AFFILIATES.  All rights reserved.

This container image and its contents are governed by the NVIDIA Deep Learning Container License.
By pulling and using the container, you accept the terms and conditions of this license:
https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license

WARNING: The NVIDIA Driver was not detected.  GPU functionality will not be available.
   Use the NVIDIA Container Toolkit to start this container with GPU support; see
   https://docs.nvidia.com/datacenter/cloud-native/ .

W0206 06:33:15.417728 1 pinned_memory_manager.cc:236] Unable to allocate pinned system memory, pinned memory pool will not be available: CUDA driver version is insufficient for CUDA runtime version
I0206 06:33:15.417810 1 cuda_memory_manager.cc:115] CUDA memory pool disabled
I0206 06:33:15.419118 1 model_lifecycle.cc:462] loading: onnx-model-test:1
I0206 06:33:15.419169 1 model_lifecycle.cc:462] loading: onnx-model-test:2
I0206 06:33:15.420559 1 onnxruntime.cc:2504] TRITONBACKEND_Initialize: onnxruntime
I0206 06:33:15.420594 1 onnxruntime.cc:2514] Triton TRITONBACKEND API version: 1.12
I0206 06:33:15.420599 1 onnxruntime.cc:2520] 'onnxruntime' TRITONBACKEND API version: 1.12
I0206 06:33:15.420604 1 onnxruntime.cc:2550] backend configuration:
{"cmdline":{"auto-complete-config":"true","backend-directory":"/opt/tritonserver/backends","min-compute-capability":"6.000000","default-max-batch-size":"4"}}
I0206 06:33:15.434784 1 onnxruntime.cc:2608] TRITONBACKEND_ModelInitialize: onnx-model-test (version 1)
I0206 06:33:15.434828 1 onnxruntime.cc:2608] TRITONBACKEND_ModelInitialize: onnx-model-test (version 2)
I0206 06:33:15.435350 1 onnxruntime.cc:666] skipping model configuration auto-complete for 'onnx-model-test': inputs and outputs already specified
I0206 06:33:15.435355 1 onnxruntime.cc:666] skipping model configuration auto-complete for 'onnx-model-test': inputs and outputs already specified
I0206 06:33:15.435790 1 onnxruntime.cc:2651] TRITONBACKEND_ModelInstanceInitialize: onnx-model-test_0 (CPU device 0)
I0206 06:33:15.435803 1 onnxruntime.cc:2651] TRITONBACKEND_ModelInstanceInitialize: onnx-model-test_0 (CPU device 0)
I0206 06:33:15.442542 1 onnxruntime.cc:2651] TRITONBACKEND_ModelInstanceInitialize: onnx-model-test_1 (CPU device 0)
I0206 06:33:16.076472 1 onnxruntime.cc:2651] TRITONBACKEND_ModelInstanceInitialize: onnx-model-test_1 (CPU device 0)
I0206 06:33:16.476478 1 model_lifecycle.cc:815] successfully loaded 'onnx-model-test'
I0206 06:33:16.476605 1 server.cc:582]
+------------------+------+
| Repository Agent | Path |
+------------------+------+
+------------------+------+

I0206 06:33:16.476653 1 server.cc:609]
+-------------+-----------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Backend     | Path                                                            | Config                                                                                                                                                        |
+-------------+-----------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------+
| onnxruntime | /opt/tritonserver/backends/onnxruntime/libtriton_onnxruntime.so | {"cmdline":{"auto-complete-config":"true","backend-directory":"/opt/tritonserver/backends","min-compute-capability":"6.000000","default-max-batch-size":"4"}} |
+-------------+-----------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------+

I0206 06:33:16.476690 1 server.cc:652]
+-----------------+---------+--------+
| Model           | Version | Status |
+-----------------+---------+--------+
| onnx-model-test | 1       | READY  |
| onnx-model-test | 2       | READY  |
+-----------------+---------+--------+

I0206 06:33:16.476976 1 metrics.cc:701] Collecting CPU metrics
I0206 06:33:16.477197 1 tritonserver.cc:2385]
+----------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Option                           | Value                                                                                                                                                                                                           |
+----------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| server_id                        | triton                                                                                                                                                                                                          |
| server_version                   | 2.34.0                                                                                                                                                                                                          |
| server_extensions                | classification sequence model_repository model_repository(unload_dependents) schedule_policy model_configuration system_shared_memory cuda_shared_memory binary_tensor_data parameters statistics trace logging |
| model_repository_path[0]         | /mnt/models                                                                                                                                                                                                     |
| model_control_mode               | MODE_NONE                                                                                                                                                                                                       |
| strict_model_config              | 0                                                                                                                                                                                                               |
| rate_limit                       | OFF                                                                                                                                                                                                             |
| pinned_memory_pool_byte_size     | 268435456                                                                                                                                                                                                       |
| min_supported_compute_capability | 6.0                                                                                                                                                                                                             |
| strict_readiness                 | 1                                                                                                                                                                                                               |
| exit_timeout                     | 30                                                                                                                                                                                                              |
| cache_enabled                    | 0                                                                                                                                                                                                               |
+----------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

```

- API Call Test (https://kserve.github.io/website/master/modelserving/data_plane/data_plane/#v2-apis)

```py
import requests
import numpy as np
from PIL import Image
from tensorflow.keras.datasets import mnist
import matplotlib.pyplot as plt

# MNIST 데이터셋 로드
(x_train, y_train), (x_test, y_test) = mnist.load_data()
test_image = x_test[1]  # 첫 번째 테스트 이미지 선택

# 테스트 이미지 전처리
image_array = np.array(test_image).astype(np.float32) / 255.0  # 정규화
image_array = np.expand_dims(image_array, axis=0)  # 배치 차원 추가

# KServe 서비스 엔드포인트 (v2 프로토콜)
# url = 'http://inf-onnx-model-test.kubeflow-user-example-com.svc.cluster.local/v2/models/onnx-model-test/versions/1/infer'
url = 'http://inf-onnx-model-test.kubeflow-user-example-com.svc.cluster.local/v2/models/onnx-model-test/versions/2/infer'

# 요청 데이터 준비
data = {
    "inputs": [{
        "name": "x",  # 모델 입력 이름
        "shape": image_array.shape,
        "datatype": "FP64",
        "data": image_array.flatten().tolist()  # 1D 리스트로 변환
    }]
}

# API 호출
response = requests.post(url, json=data)

# 예측 결과 출력 및 시각화
if response.status_code == 200:
    result = response.json()
    # print(f"Prediction: {result}")
    
    # 예측된 확률
    probabilities = result['outputs'][0]['data']
    
    # 가장 높은 확률을 가진 클래스 찾기
    predicted_class = np.argmax(probabilities)
    predicted_probability = probabilities[predicted_class]
    
    print(f"Predicted Class: {predicted_class}, Probability: {predicted_probability}")

    # 이미지 시각화
    plt.imshow(test_image, cmap='gray')
    plt.title(f'Predicted Class: {predicted_class} (Probability: {predicted_probability:.4f})')
    plt.axis('off')
    plt.show()

else:
    print(f"Request failed with status code {response.status_code}")
```